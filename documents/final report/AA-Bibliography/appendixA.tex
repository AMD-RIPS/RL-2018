\chapter{Algorithms Outline}\label{Algorithms}
\section{Basic Q-Learning Algorithm Outline}
\begin{algorithm}[H]
 Initialize $Q(s,a)$ for all $s \in S, \;\; a \in A$\\
 \For{each episode}{
 Get initial state $s_t$\\
 \While{episode is not over}{
	Sample $U \sim Unif(0,1)$ \\
	\eIf{$U < \epsilon$}{
		$a_{t} =  \textrm{random action}$
	}{
	  	$a_{t} =  \underset{a'}\argmax \; Q(s_t,a')$
	}
	Take action $a_t$, get reward $R_{t+1}$ and new state $s_{t+1}$ \\
	Set
		$Q(s_t, a_t) = Q(s_t,a_t) +  \alpha\left(R_{{t+1}} + \gamma\;\underset{a'}\max \; Q(s_{t+1},a') - Q(s_t, a_t)\right)$\\
	$\; s_t = s_{t+1}$
 }
 }
 \caption{Basic Q-learning}
 \label{alg:basic_q}
\end{algorithm}

\newpage
\section{Deep Q-Learning Algorithm Outline}
\begin{algorithm}[H]
Initialise $Q$ with random weights $\theta$\\
Set $\theta^- = \theta$. \\
 \For{each episode}{
 Get initial state $s_0$\\
	 \For{t = 1, 2, 3 ... T}{
		Sample $U \sim Unif(0,1)$ \\
		Set $a_t = \begin{dcases}
		    \text{random action sampled uniformly} & \text{if U $< \epsilon$}\\
		    \underset{a'}\argmax \; Q(s_t,a')              & \text{otherwise}
		\end{dcases}$ \\
		Take action $a_t$, get reward $R_{t+1}$ and new state $s_{t+1}$ \\
		Store $(s_t, a_t, R_{t+1}, s_{t+1})$ in the replay memory $\mathcal{D}$\\
		Sample a random minibatch of transitions $(s_j, a_j, R_{j+1}, s_{j+1})$ from $\mathcal{D}$\\
		Set $y_j = \begin{dcases}
		    R_{j+1} + \gamma\, \max\limits_{a'} Q(s_{j+1}, a';\theta^-)& \text{if $s_{j+1}$ is not terminal} \\
		    R_{j+1},              & \text{otherwise}
		\end{dcases}$ \\
		Perform a gradient descent step on the loss $\left( y_j - Q(s_j, a_j;\theta)\right)^2$ \\
		Every C steps set $\theta^- = \theta$. 
 	}
 }
\caption{Deep Q-learning}
\label{alg:deep_q}
\end{algorithm}

\endinput
